#!/usr/bin/env python3
# -*- coding:utf-8 -*-

import os
import signal
import argparse
import json
import tempfile
import shutil
import tarfile
import codecs
import uuid

DEFAULT_PIPELINE = [
    [
      'agora_results.pipes.results.do_tallies',
      {}
    ],
    [
      "agora_results.pipes.sort.sort_non_iterative",
      {
        "question_indexes": [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
      }
    ]
]

# By default we only allow the most used pipes to reduce default attack surface
# NOTE: keep the list sorted
DEFAULT_PIPES_WHITELIST = [
    #"agora_resuts.pipes.duplicate_questions.duplicate_questions",
    "agora_resuts.pipes.modifications.apply_modifications",
    #"agora_resuts.pipes.multipart.make_multipart",
    #"agora_resuts.pipes.multipart.election_max_size_corrections",
    #"agora_resuts.pipes.multipart.question_totals_with_corrections",
    #"agora_resuts.pipes.multipart.reduce_answers_with_corrections",
    #"agora_resuts.pipes.multipart.multipart_tally_plaintexts_append_joiner",
    #"agora_resuts.pipes.multipart.data_list_reverse",
    #"agora_resuts.pipes.multipart.multipart_tally_plaintexts_joiner",
    #"agora_resuts.pipes.multipart.append_ballots",
    "agora_resuts.pipes.parity.proportion_rounded",
    "agora_resuts.pipes.parity.parity_zip_non_iterative",
    #"agora_resuts.pipes.parity.reorder_winners",
    #"agora_resuts.pipes.parity.podemos_parity_loreg_zip_non_iterative",
    #"agora_resuts.pipes.podemos.podemos_proportion_rounded_and_duplicates",
    #"agora_resuts.pipes.pretty_print.pretty_print_stv_winners",
    "agora_resuts.pipes.pretty_print.pretty_print_not_iterative",
    #"agora_resuts.pipes.results.do_tallies",
    #"agora_resuts.pipes.results.to_files",
    #"agora_resuts.pipes.results.apply_removals",
    "agora_resuts.pipes.sort.sort_non_iterative",
    #"agora_resuts.pipes.stv_tiebreak.stv_first_round_tiebreak"
]

def extract_tally(fpath):
    '''
    extracts the tally and loads the results into a file for convenience
    '''
    extract_dir = tempfile.mkdtemp()
    tar = tarfile.open(fpath)
    tar.extractall(extract_dir)
    tar.close()
    return extract_dir

def print_csv(data, separator):
    counts = data['results']['questions']
    for question, i in zip(counts, range(len(counts))):
        if question['tally_type'] not in ["plurality-at-large", "borda", "borda-nauru"] or\
           question.get('no-tally', False):
            continue

        print(separator.join(["Question"]))
        print(separator.join(["Number", "Title"]))
        print(separator.join([str(i + 1), question['title']]))
        print(separator*2)

        print(separator.join(["Totals"]))

        question_total = (question['totals']['null_votes']
            + question['totals']['blank_votes']
            + question['totals']['valid_votes'])

        print(separator.join(["Total votes", str(question_total)]))
        print(separator.join(["Blank votes", str(question['totals']['blank_votes'])]))
        print(separator.join(["Null votes", str(question['totals']['null_votes'])]))
        print(separator.join(["Valid votes", str(question['totals']['valid_votes'])]))
        print(separator*2)

        print(separator.join(["Answers"]))
        print(separator.join(["Id", "Text", "Category", "Total votes", "Winner position"]))
        for answer in question['answers']:
            print(separator.join([
                str(answer['id']),
                answer['text'],
                answer['category'],
                str(answer['total_count']),
                str(answer['winner_position'])]))


def pretty_print(data):
    from agora_results.pipes.pretty_print import pretty_print_not_iterative
    pretty_print_not_iterative([data])

def print_results(data, output_format):
  '''
  print results in the specified output format
  '''
  if output_format == "json":
    print(json.dumps(
        data['results'],
        indent=4,
        ensure_ascii=False,
        sort_keys=True,
        separators=(',', ': ')))
  elif output_format == "csv":
      print_csv(data, separator=",")
  elif output_format == "tsv":
      print_csv(data, separator="\t")
  elif output_format == "pretty":
      pretty_print(data)

def func_path_sanity_checks(func_path, pipes_whitelist):
    '''
    Check that the func path is valid and reasonably secure
    '''
    if pipes_whitelist is not None and func_path not in pipes_whitelist:
        raise Exception("Pipe not in the whitelist")

    values = func_path.split(".")
    if " " in func_path or len(values) == 0 or len(values) > 4 or\
        values[0] != "agora_results" or values[1] != "pipes":
        raise Exception()

    for val in values:
        if len(val) == 0 or val.startswith("_"):
            raise Exception()

def execute_pipeline(pipeline_info, data_list, pipes_whitelist=None):
    '''
    Execute a pipeline of options. pipeline_info must be a list of
    pairs. Each pair contains (pipe_func_path, params), where pipe_func_path is
    the path to the module and a function name, and params is either
    None or a dictionary with extra parameters accepted by the function.

    Pipeline functions must accept always at least one parameter, 'data', which
    will initially be the data parameter of this function, but each function is
    allow to modify it as a way to process the data.

    The other parameters of the function will be the "params" specified for
    that function in 'pipeline_info', which can either be None or a dict, and
    the format is of your choice as a developer.
    '''
    for func_path, kwargs in pipeline_info:
        # get access to the function
        func_path_sanity_checks(func_path, pipes_whitelist)
        func_name = func_path.split(".")[-1]
        module = __import__(
            ".".join(func_path.split(".")[:-1]), globals(), locals(),
            [func_name], 0)
        fargs = dict(data_list=data_list)
        if kwargs is not None:
            fargs.update(kwargs)
        ret = getattr(module, func_name)(**fargs)

    return data_list

def read_file(path):
    with codecs.open(path, 'r', encoding="utf-8") as f:
        return f.read()

def write_file(path, data):
    with codecs.open(path, 'w', encoding="utf-8") as f:
        f.write(data)

def serialize(data):
    return json.dumps(data,
        indent=4, ensure_ascii=False, sort_keys=True, separators=(',', ': '))

def create_ephemeral_tally(econfig_path):
    '''
    Creates a tally in a temporal directory from an election config
    '''
    tmp_dir = tempfile.mkdtemp()
    econfig_txt = read_file(econfig_path)
    econfig = json.loads(econfig_txt)

    write_file(
        os.path.join(tmp_dir, "questions_json"),
        serialize(econfig["questions"]))

    for i in range(len(econfig["questions"])):
        session_id = "%d-%s" % (i, str(uuid.uuid4()))
        os.mkdir(os.path.join(tmp_dir, session_id))
        write_file(os.path.join(tmp_dir, session_id, "plaintexts_json"), "")

    return tmp_dir

def main():
    parser = argparse.ArgumentParser(description='Process and show tally '
                                     'results. If no config is specified, it '
                                     'parses results in raw.')
    parser.add_argument('-t', '--tally', nargs='*', help='tally path', default=[])
    parser.add_argument('-e', '--election-config', nargs='*', help='Instead of specifying a tally, you can specify an json election config and an empty ephemeral tally with zero votes will be created. recommended to use together with the multipart.append_ballots pipe.', default=[])
    parser.add_argument('-x', '--tar',   nargs='?', help='tar tallies output path')
    parser.add_argument('-p', '--pipes-whitelist', help='path to the file containing the allowed pipes')
    parser.add_argument('-c', '--config', help='config path')
    parser.add_argument('-s', '--stdout', help='print output to stdout',
        action='store_true')
    parser.add_argument('-o', '--output-format', help='select the output format',
        default="json", choices=["json", "csv", "tsv", "pretty", "none"])
    pargs = parser.parse_args()

    # load config
    if pargs.config is not None:
      with codecs.open(pargs.config, 'r', encoding="utf-8") as f:
          pipeline_info = json.loads(f.read())
    else:
      pipeline_info = DEFAULT_PIPELINE

    # load allowed pipes: Format of the file should simply be: one pipe per line
    if pargs.pipes_whitelist is not None:
        with codecs.open(pargs.pipes_whitelist, 'r', encoding="utf-8") as f:
            pipes_whitelist = [l.strip() for l in f.readlines()]
    else:
      pipes_whitelist = DEFAULT_PIPES_WHITELIST

    data_list = []

    # remove files on abrupt external exit signal
    def sig_handler(signum, frame):
        if not pargs.stdout:
            print("\nTerminating: deleting temporal files..")
        for data in data_list:
            if os.path.exists(data["extract_dir"]):
                shutil.rmtree(data["extract_dir"])
        exit(1)
    signal.signal(signal.SIGTERM, sig_handler)
    signal.signal(signal.SIGINT, sig_handler)

    try:
        if len(pargs.tally) == 0 and len(pargs.election_config) == 0:
            print("You need to specify at least one tally or election-config")
            exit(1)

        if len(pargs.tally) > 0 and len(pargs.election_config) > 0:
            print("You can't specify both a tally and an election-config")
            exit(1)

        if len(pargs.election_config) > 0:
            for econfig_path in pargs.election_config:
                tmp_dir = create_ephemeral_tally(econfig_path)
                if not pargs.stdout:
                    print("Ephemeral tally for config %s in %s.." %
                        (econfig_path, tmp_dir))
                data_list.append(dict(extract_dir=tmp_dir))
        else:
            # extract each tally, before starting the pipeline, and put the tally
            # relevant data in a list that is processed by the pipeline
            for tally in pargs.tally:
                extract_dir = extract_tally(tally)
                if not pargs.stdout:
                    print("Extracted tally %s in %s.." % (tally, extract_dir))
                data_list.append(dict(extract_dir=extract_dir))

        execute_pipeline(pipeline_info, data_list,
                         pipes_whitelist=pipes_whitelist)
        if pargs.stdout and len(data_list) > 0 and 'results' in data_list[0]:
          print_results(data_list[0], pargs.output_format)
        data = ""

        # tar tallies
        if pargs.tar:
            from agora_results.utils.tallies import tar_tallies
            for tally in pargs.tally:
                tar_tallies(data_list[0], pipeline_info, tally, pargs.tar)

    finally:
        if not pargs.stdout:
            print("Deleting temporal files..")
        for data in data_list:
            if os.path.exists(data["extract_dir"]):
                shutil.rmtree(data["extract_dir"])

if __name__ == '__main__':
    main()
